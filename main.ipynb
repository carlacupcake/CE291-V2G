{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from env_test import GridEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GridEnvironment class\n",
    "N = 10 # 10 EVs\n",
    "\n",
    "# Get demand data\n",
    "# TODO get apparent power from real and reactive, use just real for now\n",
    "with open('Building Load Data/real_power_data.json', 'r') as json_file:\n",
    "    real_dict = json.load(json_file)\n",
    "    for key in real_dict.keys(): # get data from first key only (CAPTL_WF)\n",
    "        demand_data = np.array(real_dict[key])\n",
    "        break\n",
    "\n",
    "# Get solar data\n",
    "with open('PV Generation Data/pv_data.json', 'r') as json_file:\n",
    "    pv_dict = json.load(json_file)\n",
    "    for key in pv_dict.keys(): # get data from first key only (CAPTL_WF)\n",
    "        solar_data = np.array(pv_dict[key])\n",
    "        break\n",
    "\n",
    "# Get wind data\n",
    "with open('Wind Data/wind_data.json', 'r') as json_file:\n",
    "    wind_dict = json.load(json_file)\n",
    "    for key in wind_dict.keys(): # get data from first key only (CAPTL_WF)\n",
    "        wind_data = np.array(wind_dict[key])\n",
    "        break\n",
    "\n",
    "grid  = GridEnvironment(N, demand_data, solar_data, wind_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427, 96)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solar_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000  # Number of episodes to run the training\n",
    "episode_hours = 24  # Duration of each episode in hours\n",
    "timestep_length = 0.25  # Duration of each timestep in hours (15 minutes)\n",
    "episode_length = int(episode_hours / timestep_length)  # Total timesteps per episode\n",
    "data=0 #TWEAK\n",
    "env = GridEnvironment(N=10, timestep_length=timestep_length, data=data)  # Initialize your environment with timestep length\n",
    "state_size = 3 + N  # State includes demand, solar, wind, and N EVs\n",
    "action_size = 3**N  # Each EV can be in one of three states (-1, 0, 1)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    day_of_year = 0# Cycling through data day by day\n",
    "    state = env.reset(day=day_of_year)\n",
    "    state = np.reshape(state, [1, state_size]) #how do i initialize from data????\n",
    "    total_reward = 0  # Initialize the total reward for the episode\n",
    "\n",
    "    for time in range(episode_length):  # Iterate over each timestep within an episode\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -1000  # Negative reward on done (optional)\n",
    "        total_reward += reward  # Accumulate reward for the episode\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode: {e+1}/{EPISODES}, Reward: {total_reward}, Steps: {time + 1}\")\n",
    "            break  # Exit the loop when the episode is done\n",
    "\n",
    "    # Optional: Adjust the learning process based on the performance, e.g., decrease epsilon\n",
    "    agent.adjust_epsilon()\n",
    "\n",
    "    # Save model at certain intervals\n",
    "    if (e + 1) % 100 == 0:\n",
    "        agent.save_model(f\"model_checkpoint_episode_{e+1}.h5\")\n",
    "\n",
    "# Save the final model after all episodes are completed\n",
    "agent.save_model(\"final_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('info251')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2064303d263e1a3ceaaf815227a08219bcf9396a8cb60113be5ce3b525cf3c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
