{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from env_test import GridEnvironment\n",
    "from DQL_agent import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "N = 10 # 10 EVs\n",
    "\n",
    "# Get demand data\n",
    "# TODO get apparent power from real and reactive, use just real for now\n",
    "with open('Building Load Data/real_power_data.json', 'r') as json_file:\n",
    "    real_dict = json.load(json_file)\n",
    "    for key in real_dict.keys(): # get data from first key only (CAPTL_WF)\n",
    "        demand_data = np.array(real_dict[key])\n",
    "        break\n",
    "\n",
    "# Get solar data\n",
    "with open('PV Generation Data/pv_data.json', 'r') as json_file:\n",
    "    pv_dict = json.load(json_file)\n",
    "    for key in pv_dict.keys(): # get data from first key only (CAPTL_WF)\n",
    "        solar_data = np.array(pv_dict[key])\n",
    "        break\n",
    "\n",
    "# Get wind data\n",
    "with open('Wind Data/wind_data.json', 'r') as json_file:\n",
    "    wind_dict = json.load(json_file)\n",
    "    for key in wind_dict.keys(): # get data from first key only (CAPTL_WF)\n",
    "        wind_data = np.array(wind_dict[key])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 96 is out of bounds for axis 1 with size 96",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timestep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Assume T timesteps in a day\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)  \u001b[38;5;66;03m# Decide action based on current state\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     reward, done, next_demand, next_solar, next_wind, next_P_EV \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Construct the new state from the separated components\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m     23\u001b[0m         np\u001b[38;5;241m.\u001b[39marray([next_demand, next_solar, next_wind]),  \u001b[38;5;66;03m# Assuming these are scalar values\u001b[39;00m\n\u001b[1;32m     24\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(next_P_EV)  \u001b[38;5;66;03m# Assuming this is already an array or list\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     ])\n",
      "File \u001b[0;32m~/Downloads/CE291/Final_Project/CE291-V2G/env_test.py:104\u001b[0m, in \u001b[0;36mGridEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_timestep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    103\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_timestep \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_timesteps\n\u001b[0;32m--> 104\u001b[0m next_demand, next_solar, next_wind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_state()\n\u001b[1;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m reward, done, next_demand, next_solar, next_wind , next_P_EV\n",
      "File \u001b[0;32m~/Downloads/CE291/Final_Project/CE291-V2G/env_test.py:116\u001b[0m, in \u001b[0;36mGridEnvironment.get_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Access the current timestep's data correctly\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     current_demand \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdemand_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_timestep]\n\u001b[1;32m    117\u001b[0m     current_solar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolar_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_timestep]\n\u001b[1;32m    118\u001b[0m     current_wind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwind_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mday_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_timestep]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 96 is out of bounds for axis 1 with size 96"
     ]
    }
   ],
   "source": [
    "# Initialize environment with your chosen day's data\n",
    "day_index=0\n",
    "timestep_length=.25 #in hours\n",
    "batch_size = 32 #TWEAk\n",
    "env = GridEnvironment(N, demand_data, solar_data, wind_data, day_index, timestep_length)\n",
    "\n",
    "# Initialize DQN agent\n",
    "agent = DQNAgent(state_size=env.state_size, action_size=env.action_size)\n",
    "\n",
    "# Run for one episode (one day)\n",
    "state = env.get_state()  # Reset environment to start state\n",
    "total_reward = 0\n",
    "\n",
    "T=96 #(15 minute steps in 24 hrs)\n",
    "\n",
    "for timestep in range(1, T+1):  # Assume T timesteps in a day\n",
    "    action = agent.act(state)  # Decide action based on current state\n",
    "\n",
    "    reward, done, next_demand, next_solar, next_wind, next_P_EV = env.step(action)\n",
    "\n",
    "    # Construct the new state from the separated components\n",
    "    next_state = np.concatenate([\n",
    "        np.array([next_demand, next_solar, next_wind]),  # Assuming these are scalar values\n",
    "        np.array(next_P_EV)  # Assuming this is already an array or list\n",
    "    ])\n",
    "    \n",
    "    agent.remember(state, action, reward, next_state, done)  # Store experience\n",
    "    #Replay not working?\\\n",
    "    #if len(agent.memory) >= batch_size:\n",
    "        #agent.replay(batch_size) # Train the agent with a minibatch from memory\n",
    "\n",
    "    state = next_state  # Move to the next state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Total reward for the episode (day):\", total_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('info251')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2064303d263e1a3ceaaf815227a08219bcf9396a8cb60113be5ce3b525cf3c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
